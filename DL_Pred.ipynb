{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNt0E5p4kQcLHIZi+1ywtV1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k5OoSzYAcXR-","executionInfo":{"status":"ok","timestamp":1738915135792,"user_tz":-540,"elapsed":7996,"user":{"displayName":"임재이","userId":"08351085303596207679"}},"outputId":"2ec429cb-e72e-41a8-871d-99452bbe7383"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import csv\n","from google.colab import drive\n","import pandas as pd\n","!pip install rouge\n","!pip install scikit-learn\n","!pip install sentence-transformers scikit-learn rouge nltk\n","\n","# 주소 세팅하기\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import resnet18\n","from transformers import BertTokenizer, BertModel\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","from torchvision import transforms\n","from sentence_transformers import SentenceTransformer, util\n","from rouge import Rouge\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.translate.bleu_score import sentence_bleu\n","import numpy as np\n","\n","# Google Drive 만용트\n","drive.mount('/content/drive')\n","\n","# CSV 파일 경로\n","csv_file_path = \"/content/drive/MyDrive/paper modeling/11.recomm/data/LLM추천결과.csv\"\n","\n","# CSV 파일 로드\n","data = pd.read_csv(csv_file_path, encoding='utf-8', on_bad_lines='skip')\n","\n","\n","# NaN 값 제거\n","df = data.dropna()\n","# Error 값 제거\n","df = df[~df.apply(lambda row: row.astype(str).str.contains(\"Error\").any(), axis=1)]\n","\n"]},{"cell_type":"code","source":["print(df.head())\n","print(df.dtypes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4jvZ--M8HYyr","executionInfo":{"status":"ok","timestamp":1738915140814,"user_tz":-540,"elapsed":20,"user":{"displayName":"임재이","userId":"08351085303596207679"}},"outputId":"d429c2fd-c016-4686-b968-5fe36ed257a5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["  사용자ID  나이   성별                                   구매상품명  \\\n","0   ID6  23   남성           Clinique For Men Skincare Set   \n","1   ID8  32   남성            NIVEA Men After Shave Lotion   \n","2   ID9  27   여성     Urban Decay Naked Eyeshadow Palette   \n","3  ID11  26   여성                      MAC Matte Lipstick   \n","4  ID12  31   남성   Neutrogena Deep Clean Facial Cleanser   \n","\n","                                                상품설명  \\\n","0   A skincare set that helps soothe and care for...   \n","1   Aftershave that helps soothe and moisturize t...   \n","2   An eyeshadow palette with a variety of vibran...   \n","3                 A luxurious matte-finish lipstick.   \n","4   A facial cleanser that deeply cleanses the skin.   \n","\n","                                                 이미지    매출액  \\\n","0   /content/drive/MyDrive/paper modeling/11.reco...  15000   \n","1   /content/drive/MyDrive/paper modeling/11.reco...  70000   \n","2   /content/drive/MyDrive/paper modeling/11.reco...  30000   \n","3   /content/drive/MyDrive/paper modeling/11.reco...  22000   \n","4   /content/drive/MyDrive/paper modeling/11.reco...  55000   \n","\n","                                                  리뷰  평점              검색기록  \\\n","0                 Feels good and comfortable to use.   5          Skincare   \n","1   Gentle and comfortable to use without irritat...   5        Aftershave   \n","2                Rich colors and smooth application.   4         Eyeshadow   \n","3         Long-lasting color and smooth application.   4          Lipstick   \n","4                  Removes oil and feels refreshing.   5   Facial Cleanser   \n","\n","                                              상품추천결과  \\\n","0   Kiehl's Creamy Eye Treatment with Avocado Est...   \n","1   The Art of Shaving After-Shave Balm NIVEA Men...   \n","2   Natasha Denona Glam Palette Pat McGrath Labs ...   \n","3   Charlotte Tilbury Matte Revolution Lipstick D...   \n","4   Paula's Choice C15 Super Booster Sunday Riley...   \n","\n","                                           상품추천결과RAG  \n","0   Drunk Elephant C-Tango Multivitamin Eye Cream...  \n","1   Lab Series Cooling Shave Cream Jack Black Bea...  \n","2   Too Faced Born This Way The Natural Nudes Hud...  \n","3   NARS Audacious Lipstick Tom Ford Lip Color Sa...  \n","4   Murad Rapid Age Spot and Pigment Lightening S...  \n","사용자ID        object\n","나이            int64\n","성별           object\n","구매상품명        object\n","상품설명         object\n","이미지          object\n","매출액           int64\n","리뷰           object\n","평점            int64\n","검색기록         object\n","상품추천결과       object\n","상품추천결과RAG    object\n","dtype: object\n"]}]},{"cell_type":"code","source":["\n","# 탑크 다른 프로세스 만들기 (ResNet + BERT)\n","class ProductRecommendationModel(nn.Module):\n","    def __init__(self):\n","        super(ProductRecommendationModel, self).__init__()\n","\n","        # ResNet for Image Features\n","        self.resnet = resnet18(pretrained=True)\n","        self.resnet.fc = nn.Identity()\n","\n","        # BERT for Text Features\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # Fully Connected Layers\n","        self.fc1 = nn.Linear(512 + 768 + 2, 256)  # Image (512) + Text (768) + User Info (3)\n","        self.fc_sales = nn.Linear(256, 1)\n","\n","    def forward(self, image, text, user_info):\n","        # Image Features\n","        image_features = self.resnet(image)\n","\n","        # Text Features\n","        tokens = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","        bert_output = self.bert(**tokens)\n","        text_features = bert_output.pooler_output\n","\n","        # Combine Features\n","        combined_features = torch.cat((image_features, text_features, user_info), dim=1)\n","        x = F.relu(self.fc1(combined_features))\n","        predicted_sales = self.fc_sales(x)\n","\n","        return predicted_sales\n","\n","# Image preprocessing\n","image_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","# 사용자-상품 정보를 활용할 Dataset 객체 정의\n","csv_path = \"/content/drive/MyDrive/paper modeling/11.keyword/data/LLM추천결과.csv\"\n","class InteractionDataset(Dataset):\n","    def __init__(self, csv_path, image_transform=None):\n","        self.data = pd.read_csv(csv_path)\n","        self.transform = image_transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","\n","        # 사용자 정보\n","        user_id = row['사용자ID']\n","        user_age = row['나이']\n","        user_gender = row['성별']\n","        # 상품 명 및 설명\n","        product_name = row['구매상품명']\n","        product_description = row['상품설명']\n","        # 상품 이미지 채워넣기\n","        image_path = row['이미지']\n","        image = Image.open(image_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        # 매출액\n","        salescount = row['매출액']\n","        review = row['리뷰']\n","        reviewscore = row['평점']\n","        search_history = row['검색기록']\n","        recomm = row['상품추천결과']\n","\n","        other_info = torch.tensor([\n","            user_age,\n","            reviewscore\n","        ], dtype=torch.float32 , requires_grad=True)\n","        # nan 값 처리\n","        other_info = torch.nan_to_num(other_info, nan=0.0, posinf=1e6, neginf=-1e6)\n","\n","\n","        return user_id, user_age, user_gender, product_name, product_description,image_path, image, salescount, review, reviewscore, search_history,recomm,other_info\n","\n","\n"],"metadata":{"id":"zqQL0T51cbeF","executionInfo":{"status":"ok","timestamp":1738916022572,"user_tz":-540,"elapsed":66,"user":{"displayName":"임재이","userId":"08351085303596207679"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["\n","###########################################################################################\n","# 예측을 수행할CSV 파일 경로 -> dataset , dataloader에서 불러옴\n","###########################################################################################\n","csv_path = \"/content/drive/MyDrive/paper modeling/11.recomm/data/LLM추천결과.csv\"\n","\n","# Create dataset and dataloader\n","dataset = InteractionDataset(csv_path, image_transform=image_transform)\n","dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n","for user_id, user_age, user_gender, product_name, product_description, image_path, image, salescount, review, reviewscore, search_history, recomm, other_info in dataloader:\n","    #print(\"Images batch:\", images.size())\n","    print(\"product_name  :\", product_name)\n","\n","# Initialize model\n","model = ProductRecommendationModel()\n","\n","# Training loop (simplified)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.MSELoss()  # For continuous value prediction (sales)\n","#criterion_sex = nn.CrossEntropyLoss()\n","#criterion_age = nn.CrossEntropyLoss()\n","\n","###########################################################################################\n","# 예측을 수행할CSV 파일 dataset , dataloader에서 불러옴\n","###########################################################################################\n","# 결과 저장용 리스트\n","results = []\n","\n","target_mae = 5\n","epochs = 100  #lr=1e-3\n","for epoch in range(epochs):\n","    total_samples = 0\n","    total_loss = 0\n","    total_absolute_error = 0  # Mean Absolute Error (MAE)\n","\n","    # 엑셀값 한줄씩 배열로 데이터제공\n","    for user_id, user_age, user_gender, product_name, product_description, image_path, image, salescount, review, reviewscore, search_history, recomm, other_info in dataloader:       ###예측하고싶은 csv데이터를 CustomDataset로 로드한다 (dataset을상속한 CustomDataset의 __get__item호출됨)\n","        optimizer.zero_grad()\n","\n","        ##########################################################################################################################################\n","        # LLM 추천상품의 예상매출액\n","        #############################################################################################################################################\n","        pred_sales = model(image, recomm, other_info).squeeze()\n","        salescount=salescount/1000\n","        print(\"===sales===\" + str(salescount) + \"===pred_sales===\" + str(pred_sales))\n","        total_samples += salescount.size(0)\n","\n","        # Compute loss\n","        total_loss += F.mse_loss(pred_sales, salescount, reduction=\"sum\").item()\n","        total_absolute_error += torch.sum(torch.abs(pred_sales - salescount)).item()\n","\n","        ###loss = criterion(pred_sales.squeeze(), sales)\n","        ###total_loss += loss.item()\n","\n","        # Backward pass\n","        loss = criterion(pred_sales.squeeze(), salescount.type(torch.float32))\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Predictions and accuracy\n","        # pred_sex_classes = pred_sex.argmax(dim=1)\n","        # pred_age_classes = pred_age.argmax(dim=1)\n","        # 연속값 예측은 argmax로불가, 예측-실제의 오차줄이는방법\n","        # MSE :평균제곱오차로 전체적손실 계산 (낮을수록좋음)\n","        # MAE : 예측값,실제값 차이의 절대값의 평균\n","    mse = total_loss / total_samples\n","    mae = total_absolute_error / total_samples\n","    print(f'Epoch {epoch + 1}/{epochs}, \"Evaluation - MSE: {mse:.4f}, MAE: {mae:.4f}\"')\n","\n","    ################################################################################\n","    # 학습종료시 매출액 예측수행\n","    ################################################################################\n","    # 학습 종료 조건\n","    if mae < target_mae:\n","        print(f\"학습 종료: MAE가 {target_mae:.2f} 이하로 감소했습니다.\")\n","\n","        # 모델을 평가 모드로 전환\n","        model.eval()\n","\n","        # 동일한 상태에서 결과를 비교하기 위해 그래디언트 계산 비활성화\n","        with torch.no_grad():\n","            for batch in dataloader:  # 전체 데이터셋 반복\n","                user_id, user_age, user_gender, product_name, product_description, image_path, image, salescount, review, reviewscore, search_history, recomm, other_info = batch\n","\n","                for i in range(len(salescount)):  #배열안의갯수\n","                    print(len(salescount))\n","                    print(product_name[i])\n","\n","                    # 추천데이터의 매출액예측\n","                    pred_sales = model(image, recomm, other_info).squeeze()\n","                    pred_sales = pred_sales*1000\n","\n","                    # 결과 저장\n","                    results.append({\n","                        \"사용자ID\": product_name[i].item() if isinstance(product_name[i], torch.Tensor) else product_name[i],\n","                        \"나이\": user_age[i].item() if isinstance(user_age[i], torch.Tensor) else user_age[i],\n","                        \"성별\": user_gender[i].item() if isinstance(user_gender[i], torch.Tensor) else user_gender[i],\n","                        \"구매상품명\": product_name[i].item() if isinstance(product_name[i], torch.Tensor) else product_name[i],\n","                        \"상품설명\": product_description[i].item() if isinstance(product_description[i], torch.Tensor) else product_description[i],\n","                        \"이미지\": image_path[i] if isinstance(image_path, list) else image_path,\n","                        \"매출액\": salescount[i].item() if isinstance(salescount[i], torch.Tensor) else salescount[i],\n","                        \"##예측 매출액\": pred_sales[i].item() if pred_sales.dim() > 0 else pred_sales.item(),\n","                        \"리뷰\": review[i].item() if isinstance(review[i], torch.Tensor) else review[i],\n","                        \"평점\": reviewscore[i].item() if isinstance(reviewscore[i], torch.Tensor) else reviewscore[i],\n","                        \"검색기록\": search_history[i].item() if isinstance(search_history[i], torch.Tensor) else search_history[i]\n","                    })\n","\n","            # 학습 종료\n","            break\n","\n","# 결과를 DataFrame으로 변환\n","results_df = pd.DataFrame(results)\n","# 결과 저장 경로\n","output_path = \"/content/drive/MyDrive/paper modeling/11.recomm/data/DeepL결과.xlsx\"\n","# Google Drive에 저장\n","results_df.to_excel(output_path, index=False)\n","print(f\"학습 결과가 저장되었습니다: {output_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bm6XmykAcbmh","executionInfo":{"status":"ok","timestamp":1738916166429,"user_tz":-540,"elapsed":32129,"user":{"displayName":"임재이","userId":"08351085303596207679"}},"outputId":"d1ba91e0-4c80-4c94-bb58-7f9d348c9c20"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["product_name  : (' Clinique For Men Skincare Set', ' Estée Lauder Double Wear Stay-in-Place Foundation', ' NIVEA Men After Shave Lotion', ' Urban Decay Naked Eyeshadow Palette', \" Kiehl's Ultra Facial Moisturizer\", ' MAC Matte Lipstick', ' Neutrogena Deep Clean Facial Cleanser', ' La Roche-Posay Anthelios Sunscreen SPF 50')\n","product_name  : (' American Crew Fiber Hair Gel',)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 0.1915,  0.2005,  0.2298, -0.0396,  0.2840,  0.1520,  0.2877,  0.1891],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(1.8522, grad_fn=<SqueezeBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-35-32d1cb90efa7>:48: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  total_loss += F.mse_loss(pred_sales, salescount, reduction=\"sum\").item()\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100, \"Evaluation - MSE: 2322.7844, MAE: 40.7392\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([5.6622, 6.2424, 6.6480, 5.9708, 6.9260, 5.8866, 9.0666, 7.6332],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(10.1338, grad_fn=<SqueezeBackward0>)\n","Epoch 2/100, \"Evaluation - MSE: 1800.9634, MAE: 33.9812\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 9.1460, 10.7405, 18.9398,  9.5303,  9.7592,  8.8678, 12.5693, 27.6210],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(1.9996, grad_fn=<SqueezeBackward0>)\n","Epoch 3/100, \"Evaluation - MSE: 1213.0648, MAE: 28.9807\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([12.6514, 15.4236, 27.2559, 14.0621, 13.7280, 12.6542, 25.0573, 51.5295],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(27.0191, grad_fn=<SqueezeBackward0>)\n","Epoch 4/100, \"Evaluation - MSE: 641.3981, MAE: 20.9619\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([12.5754, 17.3204, 61.1287, 13.3838, 13.8278, 12.8873, 34.2537, 45.4970],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(26.7563, grad_fn=<SqueezeBackward0>)\n","Epoch 5/100, \"Evaluation - MSE: 448.9473, MAE: 16.6536\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([-2.5557, -0.9425,  9.5708, -2.3929, -2.4054, -2.4754,  9.3316, 69.2645],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(7.9319, grad_fn=<SqueezeBackward0>)\n","Epoch 6/100, \"Evaluation - MSE: 1209.4296, MAE: 31.6303\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([17.3368, 25.4193, 60.7149, 18.9139, 19.4576, 18.3513, 67.8861, 89.1928],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(16.9200, grad_fn=<SqueezeBackward0>)\n","Epoch 7/100, \"Evaluation - MSE: 74.6826, MAE: 7.3615\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([15.2110, 43.9816, 73.2181, 18.3979, 17.5433, 15.9499, 61.6294, 77.9708],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(50.1298, grad_fn=<SqueezeBackward0>)\n","Epoch 8/100, \"Evaluation - MSE: 179.9260, MAE: 9.8120\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([14.3602, 34.4928, 74.6454, 21.9787, 18.2943, 17.3272, 60.5551, 93.9624],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(42.6533, grad_fn=<SqueezeBackward0>)\n","Epoch 9/100, \"Evaluation - MSE: 91.4645, MAE: 6.8265\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 1.1152, 21.1834, 50.8321, 19.0299,  8.0322,  6.8896, 40.3073, 77.9682],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(33.4814, grad_fn=<SqueezeBackward0>)\n","Epoch 10/100, \"Evaluation - MSE: 255.1626, MAE: 15.7915\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 7.4387, 30.7293, 58.2362, 26.2692, 13.0110, 11.9463, 46.3310, 86.9824],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(32.6331, grad_fn=<SqueezeBackward0>)\n","Epoch 11/100, \"Evaluation - MSE: 99.3109, MAE: 9.5210\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 6.2603, 34.8571, 60.3854, 21.5019, 12.7161, 12.5869, 45.6779, 82.7057],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(29.8841, grad_fn=<SqueezeBackward0>)\n","Epoch 12/100, \"Evaluation - MSE: 98.4749, MAE: 9.6881\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 4.7043, 34.4700, 61.9828, 17.4139, 12.8196, 13.3080, 44.5646, 79.3033],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(27.3198, grad_fn=<SqueezeBackward0>)\n","Epoch 13/100, \"Evaluation - MSE: 113.9247, MAE: 10.3059\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 4.0536, 30.1648, 60.9162, 17.5095, 13.3032, 12.7405, 43.8659, 80.8783],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(25.6566, grad_fn=<SqueezeBackward0>)\n","Epoch 14/100, \"Evaluation - MSE: 117.7386, MAE: 10.6916\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 4.0729, 26.8324, 59.1945, 19.2648, 14.2046, 12.1272, 45.5807, 83.8204],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(24.6946, grad_fn=<SqueezeBackward0>)\n","Epoch 15/100, \"Evaluation - MSE: 110.8143, MAE: 10.3997\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 4.5144, 27.6424, 58.9515, 21.0411, 15.1579, 12.3225, 48.3084, 84.6693],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(24.3752, grad_fn=<SqueezeBackward0>)\n","Epoch 16/100, \"Evaluation - MSE: 94.1837, MAE: 9.5297\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 5.1109, 31.5885, 60.7706, 22.0731, 15.9660, 13.3300, 50.6103, 84.6709],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(24.3261, grad_fn=<SqueezeBackward0>)\n","Epoch 17/100, \"Evaluation - MSE: 71.0362, MAE: 8.2451\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 5.6901, 35.2408, 63.4661, 22.9257, 16.7046, 14.6779, 51.5739, 86.0203],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(24.3616, grad_fn=<SqueezeBackward0>)\n","Epoch 18/100, \"Evaluation - MSE: 50.8146, MAE: 6.8958\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 6.1656, 36.9492, 65.5685, 24.3677, 17.6868, 15.9646, 51.3084, 88.7662],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(24.2780, grad_fn=<SqueezeBackward0>)\n","Epoch 19/100, \"Evaluation - MSE: 35.6140, MAE: 5.7223\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 6.5022, 36.8534, 66.6984, 26.5719, 19.1219, 17.1165, 50.7355, 91.5608],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(24.0364, grad_fn=<SqueezeBackward0>)\n","Epoch 20/100, \"Evaluation - MSE: 25.5132, MAE: 4.7640\"\n","학습 종료: MAE가 5.00 이하로 감소했습니다.\n","8\n"," Clinique For Men Skincare Set\n","8\n"," Estée Lauder Double Wear Stay-in-Place Foundation\n","8\n"," NIVEA Men After Shave Lotion\n","8\n"," Urban Decay Naked Eyeshadow Palette\n","8\n"," Kiehl's Ultra Facial Moisturizer\n","8\n"," MAC Matte Lipstick\n","8\n"," Neutrogena Deep Clean Facial Cleanser\n","8\n"," La Roche-Posay Anthelios Sunscreen SPF 50\n","1\n"," American Crew Fiber Hair Gel\n","학습 결과가 저장되었습니다: /content/drive/MyDrive/paper modeling/11.recomm/data/DeepL결과.xlsx\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"p3dNnuxFcbs5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5a7XK-qocbvq"},"execution_count":null,"outputs":[]}]}