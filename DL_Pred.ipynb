{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPoz7u/tWc2Ub28xLqXeR8E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k5OoSzYAcXR-","executionInfo":{"status":"ok","timestamp":1738509179520,"user_tz":-540,"elapsed":8990,"user":{"displayName":"임재이","userId":"08351085303596207679"}},"outputId":"ea2ed660-acbc-4128-e57f-7fcccff31df8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.47.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.27.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","  사용자ID  나이   성별                                              구매상품명  \\\n","0   ID6  23   남성                      Clinique For Men Skincare Set   \n","1   ID7  29   여성   Estée Lauder Double Wear Stay-in-Place Founda...   \n","2   ID8  32   남성                       NIVEA Men After Shave Lotion   \n","5  ID11  26   여성                                 MAC Matte Lipstick   \n","6  ID12  31   남성              Neutrogena Deep Clean Facial Cleanser   \n","\n","                         상품설명  \\\n","0   피부를 편안하게 관리할 수 있는 스킨케어 세트   \n","1      피부 결점을 커버해주는 고커버 파운데이션   \n","2      피부 진정과 보습을 도와주는 애프터쉐이브   \n","5          매트한 피니시의 고급스러운 립스틱   \n","6           피부를 깨끗하게 세정하는 클렌저   \n","\n","                                                 이미지    매출액  \\\n","0  /content/drive/MyDrive/paper modeling/11.recom...  15000   \n","1  /content/drive/MyDrive/paper modeling/11.recom...  40000   \n","2  /content/drive/MyDrive/paper modeling/11.recom...  70000   \n","5  /content/drive/MyDrive/paper modeling/11.recom...  22000   \n","6  /content/drive/MyDrive/paper modeling/11.recom...  55000   \n","\n","                  리뷰  평점                검색기록  \\\n","0       사용감이 좋고 편안해요   5           스킨케어 눈가크림   \n","1       매우 밀착되고 오래가요   4   파운데이션 쿠션 컨실러 비비크림   \n","2      자극 없이 편안한 사용감   5         애프터쉐이브 쉐이브젤   \n","5   색이 오래가고 발림성이 좋아요   4            립스틱 글로시립   \n","6     기름기가 제거되고 상쾌해요   5          클렌저 비타민C세럼   \n","\n","                                              상품추천결과  \\\n","0  \\n\\n스킨케어 눈가크림\\n\\n스킨케어 눈가크림\\n\\n스킨케어 눈가크림\\n\\n스킨케...   \n","1  \\n\\n석드는 파운데이션 쿠션 컨실러 비비크림 비비크림 비비크림 비비크림 비비크림 ...   \n","2  \\n\\n애프터쉐이브 쉐이브젤 is a product that is formulate...   \n","5  \\n\\nThe recommended product is the'MAC Matte L...   \n","6  \\n\\nWhat is the recommended product?\\n\\nWhat i...   \n","\n","                                           상품추천결과RAG  \n","0  https://www.amazon.com/Dry-Skin-Care-Anti-Agin...  \n","1  I think the most important thing to know about...  \n","2  Based on the purchased product 'NIVEA Men Afte...  \n","5   No relevant information found in search results.  \n","6  We have a lot of products from many brands. Th...  \n"]}],"source":["import os\n","import csv\n","from google.colab import drive\n","import pandas as pd\n","!pip install rouge\n","!pip install scikit-learn\n","!pip install sentence-transformers scikit-learn rouge nltk\n","\n","# 주소 세팅하기\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.models import resnet18\n","from transformers import BertTokenizer, BertModel\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","from torchvision import transforms\n","from sentence_transformers import SentenceTransformer, util\n","from rouge import Rouge\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.translate.bleu_score import sentence_bleu\n","import numpy as np\n","\n","# Google Drive 만용트\n","drive.mount('/content/drive')\n","\n","# CSV 파일 경로\n","csv_file_path = \"/content/drive/MyDrive/paper modeling/11.recomm/data/LLM추천결과.csv\"\n","\n","# CSV 파일 로드\n","df = pd.read_csv(csv_file_path)\n","\n","# NaN 값 제거\n","df = df.dropna()\n","# Error 값 제거\n","df = df[~df.apply(lambda row: row.astype(str).str.contains(\"Error\").any(), axis=1)]\n","\n","print(df.head())"]},{"cell_type":"code","source":["\n","# 탑크 다른 프로세스 만들기 (ResNet + BERT)\n","class ProductRecommendationModel(nn.Module):\n","    def __init__(self):\n","        super(ProductRecommendationModel, self).__init__()\n","\n","        # ResNet for Image Features\n","        self.resnet = resnet18(pretrained=True)\n","        self.resnet.fc = nn.Identity()\n","\n","        # BERT for Text Features\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","\n","        # Fully Connected Layers\n","        self.fc1 = nn.Linear(512 + 768 + 2, 256)  # Image (512) + Text (768) + User Info (3)\n","        self.fc_sales = nn.Linear(256, 1)\n","\n","    def forward(self, image, text, user_info):\n","        # Image Features\n","        image_features = self.resnet(image)\n","\n","        # Text Features\n","        tokens = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","        bert_output = self.bert(**tokens)\n","        text_features = bert_output.pooler_output\n","\n","        # Combine Features\n","        combined_features = torch.cat((image_features, text_features, user_info), dim=1)\n","        x = F.relu(self.fc1(combined_features))\n","        predicted_sales = self.fc_sales(x)\n","\n","        return predicted_sales\n","\n","# Image preprocessing\n","image_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","# 사용자-상품 정보를 활용할 Dataset 객체 정의\n","csv_path = \"/content/drive/MyDrive/paper modeling/11.keyword/data/LLM추천결과.csv\"\n","class InteractionDataset(Dataset):\n","    def __init__(self, csv_path, image_transform=None):\n","        self.data = pd.read_csv(csv_path)\n","        self.transform = image_transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","\n","        # 사용자 정보\n","        user_id = row['사용자ID']\n","        user_age = row['나이']\n","        user_gender = row['성별']\n","        # 상품 명 및 설명\n","        product_name = row['구매상품명']\n","        product_description = row['상품설명']\n","        # 상품 이미지 채워넣기\n","        image_path = row['이미지']\n","        image = Image.open(image_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        # 매출액\n","        salescount = row['매출액']\n","        review = row['리뷰']\n","        reviewscore = row['평점']\n","        search_history = row['검색기록']\n","        recomm = row['상품추천결과']\n","\n","        other_info = torch.tensor([\n","            user_age,\n","            reviewscore\n","        ], dtype=torch.float32 , requires_grad=True)\n","        # nan 값 처리\n","        other_info = torch.nan_to_num(other_info, nan=0.0, posinf=1e6, neginf=-1e6)\n","\n","\n","        return user_id, user_age, user_gender, product_name, product_description, image_path, image, salescount, review, reviewscore, search_history,recomm, other_info\n","\n","\n"],"metadata":{"id":"zqQL0T51cbeF","executionInfo":{"status":"ok","timestamp":1738509420052,"user_tz":-540,"elapsed":266,"user":{"displayName":"임재이","userId":"08351085303596207679"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["\n","###########################################################################################\n","# 예측을 수행할CSV 파일 경로 -> dataset , dataloader에서 불러옴\n","###########################################################################################\n","csv_path = \"/content/drive/MyDrive/paper modeling/11.recomm/data/LLM추천결과.csv\"\n","\n","# Create dataset and dataloader\n","dataset = InteractionDataset(csv_path, image_transform=image_transform)\n","dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n","for user_id, user_age, user_gender, product_name, product_description, image_path, image, salescount, review, reviewscore, search_history, recomm, other_info in dataloader:\n","    #print(\"Images batch:\", images.size())\n","    print(\"product_name  :\", product_name)\n","\n","# Initialize model\n","model = ProductRecommendationModel()\n","\n","# Training loop (simplified)\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.MSELoss()  # For continuous value prediction (sales)\n","#criterion_sex = nn.CrossEntropyLoss()\n","#criterion_age = nn.CrossEntropyLoss()\n","\n","###########################################################################################\n","# 예측을 수행할CSV 파일 dataset , dataloader에서 불러옴\n","###########################################################################################\n","# 결과 저장용 리스트\n","results = []\n","\n","target_mae = 30\n","epochs = 100  #lr=1e-3 / 5000에폭만에 원본매출액에맞는 예측에도달, [9000,9000,3000,3000] 9000까지도달할때까지, 3000이 먼저도달후 다시감소함\n","for epoch in range(epochs):\n","    total_samples = 0\n","    total_loss = 0\n","    total_absolute_error = 0  # Mean Absolute Error (MAE)\n","\n","    # 엑셀값 한줄씩 배열로 데이터제공\n","    for user_id, user_age, user_gender, product_name, product_description, image_path, image, salescount, review, reviewscore, search_history, recomm, other_info in dataloader:       ###예측하고싶은 csv데이터를 CustomDataset로 로드한다 (dataset을상속한 CustomDataset의 __get__item호출됨)\n","        optimizer.zero_grad()\n","\n","        ##########################################################################################################################################\n","        # LLM 추천상품의 예상매출액\n","        #############################################################################################################################################\n","        pred_sales = model(image, recomm, other_info).squeeze()\n","\n","        salescount = salescount / 1000\n","        print(\"===sales===\" + str(salescount) + \"===pred_sales===\" + str(pred_sales))\n","        total_samples += salescount.size(0)\n","\n","        # Compute loss\n","        total_loss += F.mse_loss(pred_sales, salescount, reduction=\"sum\").item()\n","        total_absolute_error += torch.sum(torch.abs(pred_sales - salescount)).item()\n","\n","        ###loss = criterion(pred_sales.squeeze(), sales)\n","        ###total_loss += loss.item()\n","\n","        # Backward pass\n","        loss = criterion(pred_sales.squeeze(), salescount.type(torch.float32))\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Predictions and accuracy\n","        # pred_sex_classes = pred_sex.argmax(dim=1)\n","        # pred_age_classes = pred_age.argmax(dim=1)\n","        # 연속값 예측은 argmax로불가, 예측-실제의 오차줄이는방법\n","        # MSE :평균제곱오차로 전체적손실 계산 (낮을수록좋음)\n","        # MAE : 예측값,실제값 차이의 절대값의 평균\n","    mse = total_loss / total_samples\n","    mae = total_absolute_error / total_samples\n","    print(f'Epoch {epoch + 1}/{epochs}, \"Evaluation - MSE: {mse:.4f}, MAE: {mae:.4f}\"')\n","\n","    ################################################################################\n","    # 학습종료시 매출액 예측수행\n","    ################################################################################\n","    # 학습 종료 조건\n","    if mae < target_mae:\n","        print(f\"학습 종료: MAE가 {target_mae:.2f} 이하로 감소했습니다.\")\n","\n","        # 모델을 평가 모드로 전환\n","        model.eval()\n","\n","        # 동일한 상태에서 결과를 비교하기 위해 그래디언트 계산 비활성화\n","        with torch.no_grad():\n","            for batch in dataloader:  # 전체 데이터셋 반복\n","                user_id, user_age, user_gender, product_name, product_description, image_path, image, salescount, review, reviewscore, search_history, recomm, other_info = batch\n","\n","                for i in range(len(salescount)):  #배열안의갯수\n","                    print(len(salescount))\n","                    print(product_name[i])\n","\n","                    # 추천데이터의 매출액예측\n","                    pred_sales = model(image, recomm, other_info).squeeze()\n","\n","                    # 결과 저장\n","                    results.append({\n","                        \"사용자ID\": product_name[i].item() if isinstance(product_name[i], torch.Tensor) else product_name[i],\n","                        \"나이\": user_age[i].item() if isinstance(user_age[i], torch.Tensor) else user_age[i],\n","                        \"성별\": user_gender[i].item() if isinstance(user_gender[i], torch.Tensor) else user_gender[i],\n","                        \"구매상품명\": product_name[i].item() if isinstance(product_name[i], torch.Tensor) else product_name[i],\n","                        \"상품설명\": product_description[i].item() if isinstance(product_description[i], torch.Tensor) else product_description[i],\n","                        \"이미지\": image_path[i] if isinstance(image_path, list) else image_path,\n","                        \"매출액\": salescount[i].item() if isinstance(salescount[i], torch.Tensor) else salescount[i],\n","                        \"##예측 매출액\": pred_sales[i].item() if pred_sales.dim() > 0 else pred_sales.item(),\n","                        \"리뷰\": review[i].item() if isinstance(review[i], torch.Tensor) else review[i],\n","                        \"평점\": reviewscore[i].item() if isinstance(reviewscore[i], torch.Tensor) else reviewscore[i],\n","                        \"검색기록\": search_history[i].item() if isinstance(search_history[i], torch.Tensor) else search_history[i]\n","                    })\n","\n","            # 학습 종료\n","            break\n","\n","# 결과를 DataFrame으로 변환\n","results_df = pd.DataFrame(results)\n","# 결과 저장 경로\n","output_path = \"/content/drive/MyDrive/paper modeling/11.recomm/data/매출예측결과.xlsx\"\n","# Google Drive에 저장\n","results_df.to_excel(output_path, index=False)\n","print(f\"학습 결과가 저장되었습니다: {output_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bm6XmykAcbmh","executionInfo":{"status":"ok","timestamp":1738515283161,"user_tz":-540,"elapsed":12629,"user":{"displayName":"임재이","userId":"08351085303596207679"}},"outputId":"18ce6930-edfb-41b9-f478-6ffaf0c7b9aa"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["product_name  : (' Clinique For Men Skincare Set', ' Estée Lauder Double Wear Stay-in-Place Foundation', ' NIVEA Men After Shave Lotion', ' Urban Decay Naked Eyeshadow Palette', \" Kiehl's Ultra Facial Moisturizer\", ' MAC Matte Lipstick', ' Neutrogena Deep Clean Facial Cleanser', ' La Roche-Posay Anthelios Sunscreen SPF 50')\n","product_name  : (' American Crew Fiber Hair Gel',)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 0.1145, -0.0253,  0.0685, -0.0559, -0.0576, -0.0261,  0.2741, -0.0503],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(0.6188, grad_fn=<SqueezeBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-60-ee52643d319e>:49: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  total_loss += F.mse_loss(pred_sales, salescount, reduction=\"sum\").item()\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/100, \"Evaluation - MSE: 2340.2202, MAE: 41.0155\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([2.8852, 4.0132, 6.3812, 4.4999, 4.1122, 3.5595, 6.4948, 6.8548],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(9.6681, grad_fn=<SqueezeBackward0>)\n","Epoch 2/100, \"Evaluation - MSE: 1900.8360, MAE: 35.7257\"\n","===sales===tensor([15., 40., 70., 30., 25., 22., 55., 95.])===pred_sales===tensor([ 7.3222,  9.5220, 20.8860,  8.6452,  8.4006,  7.8038, 10.7826, 25.7186],\n","       grad_fn=<SqueezeBackward0>)\n","===sales===tensor([18.])===pred_sales===tensor(17.1192, grad_fn=<SqueezeBackward0>)\n","Epoch 3/100, \"Evaluation - MSE: 1232.1126, MAE: 28.2000\"\n","학습 종료: MAE가 30.00 이하로 감소했습니다.\n","8\n"," Clinique For Men Skincare Set\n","8\n"," Estée Lauder Double Wear Stay-in-Place Foundation\n","8\n"," NIVEA Men After Shave Lotion\n","8\n"," Urban Decay Naked Eyeshadow Palette\n","8\n"," Kiehl's Ultra Facial Moisturizer\n","8\n"," MAC Matte Lipstick\n","8\n"," Neutrogena Deep Clean Facial Cleanser\n","8\n"," La Roche-Posay Anthelios Sunscreen SPF 50\n","1\n"," American Crew Fiber Hair Gel\n","학습 결과가 저장되었습니다: /content/drive/MyDrive/paper modeling/11.recomm/data/매출예측결과.xlsx\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"p3dNnuxFcbs5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5a7XK-qocbvq"},"execution_count":null,"outputs":[]}]}